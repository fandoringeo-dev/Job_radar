# Job Radar — журнал разработки (DEVLOG)

## Шаг 0 — старт и окружение
**Цель:** подготовить каркас проекта, чтобы дальше добавлять парсеры и API без хаоса.

### Что сделано
- Создана структура папок: `src/app`, `core`, `models`, `scrapers`, `config`, `storage`.
- Добавлен базовый логгер `src/app/core/logging.py`, чтобы видеть, что делает программа.
- Добавлен входной файл `src/app/main.py`.

### Зачем это нужно
- `src/app/...` — код приложения (как в реальных проектах).
- `core/` — общие вещи (логирование, конфиг, утилиты).
- `models/` — модели данных (например, вакансия).
- `scrapers/` — сборщики по источникам (каждый сайт отдельным модулем).
- `storage/` — локальные файлы/база (обычно не коммитят в git).

### Как проверить
- Запустить проект (команду уточняем после настройки pyproject).


## Шаг 0.3 — первый успешный запуск
**Проверка:** `python src/app/main.py`

**Результат:** в логах появилось `Project boot OK`, значит проект запускается и логирование работает.


## Шаг 1 — venv активен, запуск как модуля
**Проверка venv:** в терминале есть префикс `(.venv)` и `python -m app.main` запускается.

**Команда:**
`python -m app.main`

**Результат:**
`Project boot OK`



## Шаг 2 — базовый интерфейс скрейперов
**Что сделано:**
- Добавлен `app/scrapers/base.py` с интерфейсом `Scraper` и моделью `RawJob`.

**Зачем:**
- Чтобы каждый источник (hh/habr/сайт компании) подключался одинаково.
- Чтобы остальной код (сохранение в БД, API) не зависел от конкретного сайта.


## Шаг 4 — источник Habr Career (поиск)
**Что сделано:** добавлен `HabrSearchScraper`, который получает HTML страницы вакансий и извлекает title+url.

**Проверка:** `python -m app.main` печатает первые вакансии.
